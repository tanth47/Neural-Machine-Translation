{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install pyvi"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:53:34.260564Z","iopub.execute_input":"2023-05-02T06:53:34.261136Z","iopub.status.idle":"2023-05-02T06:53:48.087086Z","shell.execute_reply.started":"2023-05-02T06:53:34.261103Z","shell.execute_reply":"2023-05-02T06:53:48.085853Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkbXJCqiKQVI","executionInfo":{"status":"ok","timestamp":1683210514867,"user_tz":-420,"elapsed":7929,"user":{"displayName":"Hoàng Nguyễn Hữu","userId":"09962454182626179701"}},"outputId":"cf494506-904d-4606-ac2f-61ad121e7f9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyvi\n","  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from pyvi) (1.2.2)\n","Collecting sklearn-crfsuite\n","  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (3.1.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.10.1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->pyvi) (1.2.0)\n","Collecting python-crfsuite>=0.8.3\n","  Downloading python_crfsuite-0.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.5/993.5 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (1.16.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (0.8.10)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.10/dist-packages (from sklearn-crfsuite->pyvi) (4.65.0)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.9 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"]}]},{"cell_type":"markdown","source":["# Import Lib"],"metadata":{"id":"lzDFS9L5KQVM"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.nn.utils.rnn import pad_sequence\n","from transformers import get_linear_schedule_with_warmup\n","import re, os, time, gc, codecs, string, math, heapq\n","from typing import List\n","from pyvi import ViTokenizer\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.translate.bleu_score import corpus_bleu\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","SRC_LANGUAGE = 'eng'\n","TGT_LANGUAGE = 'vi'"],"metadata":{"id":"nRPUxQvLtSwx","execution":{"iopub.status.busy":"2023-05-02T06:53:48.089926Z","iopub.execute_input":"2023-05-02T06:53:48.091091Z","iopub.status.idle":"2023-05-02T06:53:58.562885Z","shell.execute_reply.started":"2023-05-02T06:53:48.091047Z","shell.execute_reply":"2023-05-02T06:53:58.561730Z"},"trusted":true,"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1683210552672,"user_tz":-420,"elapsed":5008,"user":{"displayName":"Hoàng Nguyễn Hữu","userId":"09962454182626179701"}},"outputId":"70a0c4b8-e6bb-4fc1-c5fa-15d08710aa79"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-1a763999fbf4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_linear_schedule_with_warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["NUM_EPOCHS = 50\n","SAMPLES = 1e9\n","MAX_LEN = 50\n","continue_training_from_checkpoint = 0 # 0: retrain\n","checkpoint_path = f'/kaggle/input/translate-machine-model-state-dict/checkpoint_{continue_training_from_checkpoint}.pth'\n","batch_size = 15\n","d_model = 512\n","num_heads = 8\n","d_ff = 512\n","batch_size = 64\n","num_encoder_layers = 6\n","num_decoder_layers = 6\n","dropout = 0.1"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:53:58.565648Z","iopub.execute_input":"2023-05-02T06:53:58.567165Z","iopub.status.idle":"2023-05-02T06:53:58.573497Z","shell.execute_reply.started":"2023-05-02T06:53:58.567117Z","shell.execute_reply":"2023-05-02T06:53:58.572485Z"},"trusted":true,"id":"oB7HWXdnKQVQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Data Preparing"],"metadata":{"id":"zoEhyJ6owUcV"}},{"cell_type":"code","source":["!git clone https://github.com/KCDichDaNgu/KC4.0_MultilingualNMT.git\n","DATA_DIR = '/kaggle/working/KC4.0_MultilingualNMT/data/iwslt_en_vi'"],"metadata":{"id":"jMCE8jiawi3L","outputId":"12e3c3dc-6028-47f2-8763-53bb7c1f52ba","execution":{"iopub.status.busy":"2023-05-02T06:53:58.577202Z","iopub.execute_input":"2023-05-02T06:53:58.578343Z","iopub.status.idle":"2023-05-02T06:54:03.313415Z","shell.execute_reply.started":"2023-05-02T06:53:58.578303Z","shell.execute_reply":"2023-05-02T06:54:03.312197Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Cloning into 'KC4.0_MultilingualNMT'...\nremote: Enumerating objects: 268, done.\u001b[K\nremote: Counting objects: 100% (13/13), done.\u001b[K\nremote: Compressing objects: 100% (10/10), done.\u001b[K\nremote: Total 268 (delta 5), reused 3 (delta 3), pack-reused 255\u001b[K\nReceiving objects: 100% (268/268), 45.75 MiB | 19.37 MiB/s, done.\nResolving deltas: 100% (68/68), done.\n","output_type":"stream"}]},{"cell_type":"code","source":["def preprocess_sentence(text, language):\n","    # Lowercase the text\n","    text = text.lower()\n","    \n","    # Remove punctuations\n","    text = text.translate(str.maketrans('', '', string.punctuation))\n","    \n","    # Handle special characters\n","#     text = re.sub(r'\\d+', 'num', text) # Replace all digits with 'num'\n","    text = re.sub(r'\\s+', ' ', text) # Replace multiple whitespaces with a single space\n","        \n","#     Tokenize the text\n","#     words = word_tokenize(text)\n","    \n","#     if language == 'eng' and len(words) > 50:\n","#         stop_words = set(stopwords.words('english'))\n","#         words = [w for w in words if not w in stop_words]\n","#     if language == 'vi' and len(words) > 50:\n","#         f = codecs.open('/kaggle/input/sentiment-analysis-foody/vietnamese-stopwords.txt', encoding='utf-8')\n","#         stop_words = []\n","#         for i, line in enumerate(f):\n","#             line = repr(line)\n","#             line = line[1:len(line)-3]\n","#             stop_words.append(line)\n","#         words = [w for w in words if not w in stop_words]\n","    # Deal with rare or infrequent words\n","    return text.strip()\n","\n","def load_data(source_file, target_file, number_of_examples, MAX_LEN):\n","    source_sents = open(source_file, \"r\").readlines()\n","    target_sents = open(target_file, \"r\").readlines()\n","    assert len(source_sents) == len(target_sents)\n","\n","    source_data, target_data = [], []\n","\n","    for src_sentence, trg_sentence in zip(source_sents, target_sents):\n","        if(len(source_data) >= number_of_examples):\n","            break\n","        if(len(src_sentence.split()) > MAX_LEN or len(trg_sentence.split())> MAX_LEN):\n","            continue\n","        source_data.append(preprocess_sentence(src_sentence, SRC_LANGUAGE))\n","        target_data.append(preprocess_sentence(trg_sentence, TGT_LANGUAGE))\n","    return source_data, target_data\n","\n","source_data, target_data = load_data(DATA_DIR+\"/train.en\", DATA_DIR+\"/train.vi\", SAMPLES, MAX_LEN)\n","src_tokenized = [word_tokenize(i) for i in source_data]\n","tgt_tokenized = [ViTokenizer.tokenize(i).split() for i in target_data] "],"metadata":{"id":"3b8Zfqrj6FEv","execution":{"iopub.status.busy":"2023-05-02T06:54:03.316666Z","iopub.execute_input":"2023-05-02T06:54:03.317037Z","iopub.status.idle":"2023-05-02T06:54:55.106339Z","shell.execute_reply.started":"2023-05-02T06:54:03.317002Z","shell.execute_reply":"2023-05-02T06:54:55.105224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tgt_tokenized[0]"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:55.107813Z","iopub.execute_input":"2023-05-02T06:54:55.108892Z","iopub.status.idle":"2023-05-02T06:54:55.118015Z","shell.execute_reply.started":"2023-05-02T06:54:55.108838Z","shell.execute_reply":"2023-05-02T06:54:55.116643Z"},"trusted":true,"id":"4D7evvjfKQVV","outputId":"4434c242-e56a-4c6a-9540-28a0ef4a497c"},"execution_count":null,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['khoa_học', 'đằng', 'sau', 'một', 'tiêu_đề', 'về', 'khí_hậu']"},"metadata":{}}]},{"cell_type":"code","source":["vocab_transform = {}\n","special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n","UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","def yield_tokens(lang_tokenized):\n","    for i in lang_tokenized:\n","        yield i\n","vocab_transform[SRC_LANGUAGE] = build_vocab_from_iterator(yield_tokens(src_tokenized), min_freq=1, specials=special_symbols, special_first=True)\n","vocab_transform[TGT_LANGUAGE] = build_vocab_from_iterator(yield_tokens(tgt_tokenized), min_freq=1, specials=special_symbols, special_first=True)\n","vocab_transform[SRC_LANGUAGE].set_default_index(UNK_IDX)\n","vocab_transform[TGT_LANGUAGE].set_default_index(UNK_IDX)\n","src_vocab_size = len(vocab_transform[SRC_LANGUAGE])\n","tgt_vocab_size = len(vocab_transform[TGT_LANGUAGE])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:55.120065Z","iopub.execute_input":"2023-05-02T06:54:55.120461Z","iopub.status.idle":"2023-05-02T06:54:56.296493Z","shell.execute_reply.started":"2023-05-02T06:54:55.120421Z","shell.execute_reply":"2023-05-02T06:54:56.295528Z"},"trusted":true,"id":"LRoshYpYKQVV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(vocab_transform[SRC_LANGUAGE]['The'])\n","print(vocab_transform[TGT_LANGUAGE]['4'])"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.298265Z","iopub.execute_input":"2023-05-02T06:54:56.298648Z","iopub.status.idle":"2023-05-02T06:54:56.304873Z","shell.execute_reply.started":"2023-05-02T06:54:56.298610Z","shell.execute_reply":"2023-05-02T06:54:56.303647Z"},"trusted":true,"id":"Z1R0PCGdKQVX","outputId":"b7f7d95a-4df0-4f16-e56b-671752aa25ae"},"execution_count":null,"outputs":[{"name":"stdout","text":"0\n394\n","output_type":"stream"}]},{"cell_type":"code","source":["print(vocab_transform[SRC_LANGUAGE](['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']))\n","print(vocab_transform[TGT_LANGUAGE](tgt_tokenized[0]))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.306627Z","iopub.execute_input":"2023-05-02T06:54:56.307917Z","iopub.status.idle":"2023-05-02T06:54:56.316050Z","shell.execute_reply.started":"2023-05-02T06:54:56.307879Z","shell.execute_reply":"2023-05-02T06:54:56.314881Z"},"trusted":true,"id":"G6TmNZ-jKQVX","outputId":"f17973bf-4d30-427f-a162-0fcce0066890"},"execution_count":null,"outputs":[{"name":"stdout","text":"[0, 1255, 2376, 5493, 7942, 123, 4, 6805, 1315]\n[209, 898, 82, 7, 2925, 30, 970]\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Create Data for Training"],"metadata":{"id":"vInPj1ZqKQVY"}},{"cell_type":"code","source":["def sequential_transform(*transforms):\n","    def func(tokens_input):\n","        for transform in transforms:\n","            tokens_input = transform(tokens_input)\n","        return tokens_input\n","    return func\n","\n","def tensor_transform(tokens_ids: List[int]):\n","    # Adding bos and eos\n","    return torch.cat((torch.tensor([BOS_IDX]), torch.tensor(tokens_ids), torch.tensor([EOS_IDX])))\n","\n","\n","text_transform = {}\n","for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n","    text_transform[ln] = sequential_transform(vocab_transform[ln], tensor_transform)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.321050Z","iopub.execute_input":"2023-05-02T06:54:56.321579Z","iopub.status.idle":"2023-05-02T06:54:56.328655Z","shell.execute_reply.started":"2023-05-02T06:54:56.321552Z","shell.execute_reply":"2023-05-02T06:54:56.327478Z"},"trusted":true,"id":"Xqj_RY4vKQVY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def collate_fn(batch):\n","    src_batch, tgt_batch = [], []\n","    # Word to index\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n","        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n","\n","    # Padding \n","    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n","    \n","    # size: [max_lenght, batch_size]\n","    return src_batch.to(torch.int64), tgt_batch.to(torch.int64)\n","\n","train_iter = [i for i in zip(src_tokenized, tgt_tokenized)]\n","train_dataloader = DataLoader(train_iter, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.330224Z","iopub.execute_input":"2023-05-02T06:54:56.330771Z","iopub.status.idle":"2023-05-02T06:54:56.377202Z","shell.execute_reply.started":"2023-05-02T06:54:56.330732Z","shell.execute_reply":"2023-05-02T06:54:56.376200Z"},"trusted":true,"id":"kLQELkTqKQVZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"id":"2OhJaiJfBDj4"}},{"cell_type":"markdown","source":["## Positional Encoding"],"metadata":{"id":"_nFA0Tt0KQVa"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super().__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","        pe = torch.zeros(max_len, d_model)\n","        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0).transpose(0, 1)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x + self.pe[:x.size(0), :]\n","        return self.dropout(x)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.379012Z","iopub.execute_input":"2023-05-02T06:54:56.379429Z","iopub.status.idle":"2023-05-02T06:54:56.388976Z","shell.execute_reply.started":"2023-05-02T06:54:56.379389Z","shell.execute_reply":"2023-05-02T06:54:56.387851Z"},"trusted":true,"id":"4dxSHpyuKQVa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Token Embedding"],"metadata":{"id":"qIy6eaWhKQVa"}},{"cell_type":"code","source":["class TokenEmbedding(nn.Module):\n","    def __init__(self, d_model, vocab_size):\n","        super(TokenEmbedding, self).__init__()\n","        self.token_embedding = nn.Embedding(vocab_size, d_model)\n","        self.d_model = d_model\n","    \n","    def forward(self, tokens):\n","        # math.sqrt(self.d_model) for stability training\n","        return self.token_embedding(tokens.long()) * math.sqrt(self.d_model)\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.390687Z","iopub.execute_input":"2023-05-02T06:54:56.391462Z","iopub.status.idle":"2023-05-02T06:54:56.399349Z","shell.execute_reply.started":"2023-05-02T06:54:56.391408Z","shell.execute_reply":"2023-05-02T06:54:56.398290Z"},"trusted":true,"id":"HA51g5JtKQVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Attention"],"metadata":{"id":"gK32tUXoKQVb"}},{"cell_type":"code","source":["# Self Attention\n","def attention(q, k, v, mask=None, dropout=None):\n","    d_k = q.size(-1)\n","    dot_product = torch.bmm(q, k.transpose(1, 2)) / math.sqrt(d_k)\n","    if mask is not None:\n","        for i in range(0, dot_product.size(0), mask.size(0)):\n","            dot_product[i: i+mask.size(0)] = (dot_product[i: i+mask.size(0)]).masked_fill(mask == 0, -1e9)\n","        scores = dot_product\n","    else:\n","        scores = dot_product\n","        \n","    p_attn = F.softmax(scores, dim=-1)\n","\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.bmm(p_attn, v)\n","# MultiHeadAttention\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_model, num_heads, dropout=0.1):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % num_heads == 0\n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.linears = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(4)])\n","        self.dropout = nn.Dropout(dropout)\n","        self.d_k = d_model // num_heads\n","\n","    def forward(self, q, k, v, mask=None):\n","\n","        q = self.linears[0](q)\n","        k = self.linears[1](k)\n","        v = self.linears[2](v)\n","        def _split_heads(tensor):\n","            bsz, length, embed_dim = tensor.size()\n","            tensor = tensor.reshape(bsz, length, self.num_heads, self.d_k).transpose(1, 2).reshape(bsz * self.num_heads, -1, self.d_k)\n","            return tensor\n","        q = _split_heads(q)\n","        k = _split_heads(k)\n","        v = _split_heads(v)\n","\n","        output = attention(q, k, v, mask=mask,  dropout=self.dropout)\n","\n","        bsz_heads, length, d_k = output.size()\n","        bsz = bsz_heads // num_heads\n","        output = output.reshape(bsz, num_heads, length, self.d_k).transpose(1, 2).reshape(bsz, length, -1)\n","\n","        return self.linears[3](output)\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.400907Z","iopub.execute_input":"2023-05-02T06:54:56.401286Z","iopub.status.idle":"2023-05-02T06:54:56.415983Z","shell.execute_reply.started":"2023-05-02T06:54:56.401250Z","shell.execute_reply":"2023-05-02T06:54:56.414797Z"},"trusted":true,"id":"aJpQySx_KQVb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## LayerNorm"],"metadata":{"id":"fsSP6AkBKQVc"}},{"cell_type":"code","source":["# Calculate LayerNorm before Skip-Connection, avoid vanishing gradient\n","# https://arxiv.org/abs/1910.05895\n","class SublayerConnection(nn.Module):\n","    def __init__(self, size, dropout):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = nn.LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        return x + self.dropout(sublayer(self.norm(x)))\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.417553Z","iopub.execute_input":"2023-05-02T06:54:56.418282Z","iopub.status.idle":"2023-05-02T06:54:56.429837Z","shell.execute_reply.started":"2023-05-02T06:54:56.418226Z","shell.execute_reply":"2023-05-02T06:54:56.428985Z"},"trusted":true,"id":"tkg0ORvQKQVc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Position-wise Feed-Forward"],"metadata":{"id":"ECknXr9zKQVc"}},{"cell_type":"code","source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(FeedForward, self).__init__()\n","        self.d_model = d_model \n","        self.d_ff = d_ff\n","        self.dropout = nn.Dropout(dropout)\n","        self.linear_in = nn.Linear(d_model, d_ff)\n","        self.linear_out = nn.Linear(d_ff, d_model)\n","        \n","    def forward(self, x):\n","        y = F.relu(self.linear_in(x))\n","        y = self.linear_out(self.dropout(y))\n","        return y\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.432518Z","iopub.execute_input":"2023-05-02T06:54:56.433293Z","iopub.status.idle":"2023-05-02T06:54:56.444444Z","shell.execute_reply.started":"2023-05-02T06:54:56.433255Z","shell.execute_reply":"2023-05-02T06:54:56.443460Z"},"trusted":true,"id":"fipNZ76UKQVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encoder"],"metadata":{"id":"nXOrJi0QKQVd"}},{"cell_type":"code","source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, d_ff, num_heads, dropout=0.1):\n","        super(EncoderLayer, self).__init__()\n","        self.SelfMultiHeadAttention = MultiHeadAttention(d_model, num_heads, dropout)\n","        self.FeedForward = FeedForward(d_model, d_ff, dropout)\n","        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n","        self.d_model = d_model\n","\n","    def forward(self, x, mask):\n","        x = self.sublayer[0](x, lambda x: self.SelfMultiHeadAttention(x, x, x, mask))\n","        return self.sublayer[1](x, self.FeedForward)\n","\n","# stack encoder layers\n","class Encoder(nn.Module):\n","    def __init__(self, d_model, d_ff, num_heads, num_layers, dropout=0.1):\n","        super(Encoder, self).__init__()\n","        self.layers = nn.ModuleList([EncoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","\n","    def forward(self, x, mask):\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)\n","\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.446869Z","iopub.execute_input":"2023-05-02T06:54:56.447226Z","iopub.status.idle":"2023-05-02T06:54:56.458466Z","shell.execute_reply.started":"2023-05-02T06:54:56.447188Z","shell.execute_reply":"2023-05-02T06:54:56.457444Z"},"trusted":true,"id":"oz6LwHF6KQVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Decoder\n"],"metadata":{"id":"Ro8ygp34KQVe"}},{"cell_type":"code","source":["class DecoderLayer(nn.Module):\n","    def __init__(self, d_model, d_ff, num_heads, dropout=0.1):\n","        super(DecoderLayer, self).__init__()\n","        self.SelfMultiHeadAttention = MultiHeadAttention(d_model, num_heads, dropout)\n","        self.MultiHeadAttention = MultiHeadAttention(d_model, num_heads, dropout)\n","        self.FeedForward = FeedForward(d_model, d_ff, dropout)\n","        self.sublayer = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n","        self.d_model = d_model\n","    \n","    def forward(self, x, memory, tgt_mask):\n","        x = self.sublayer[0](x, lambda x: self.SelfMultiHeadAttention(x, x, x, tgt_mask))\n","        x = self.sublayer[1](x, lambda x: self.MultiHeadAttention(x, memory, memory))\n","        return self.sublayer[2](x, self.FeedForward)\n","\n","# stack decoder layers\n","class Decoder(nn.Module):\n","    def __init__(self, d_model, d_ff, num_heads, num_layers, dropout=0.1):\n","        super(Decoder, self).__init__()\n","        self.layers = nn.ModuleList([DecoderLayer(d_model, d_ff, num_heads, dropout) for _ in range(num_layers)])\n","        self.norm = nn.LayerNorm(d_model)\n","    \n","    def forward(self, x, memory, tgt_mask):\n","        for layer in self.layers:\n","            x = layer(x, memory, tgt_mask)\n","        return self.norm(x)\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.460099Z","iopub.execute_input":"2023-05-02T06:54:56.460454Z","iopub.status.idle":"2023-05-02T06:54:56.473040Z","shell.execute_reply.started":"2023-05-02T06:54:56.460420Z","shell.execute_reply":"2023-05-02T06:54:56.472331Z"},"trusted":true,"id":"tD7JXf4-KQVe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer Model"],"metadata":{"id":"oRF8NtjCKQVe"}},{"cell_type":"code","source":["class Transformer(nn.Module):\n","    def __init__(self,\n","                 num_encoder_layers,\n","                 num_decoder_layers,\n","                 d_model, d_ff,\n","                 src_vocab_size,\n","                 tgt_vocab_size,\n","                 num_heads, dropout):\n","        super(Transformer, self).__init__()\n","        self.pos_encoding = PositionalEncoding(d_model, dropout)\n","        self.src_tok_emb = TokenEmbedding(d_model, src_vocab_size)\n","        self.tgt_tok_emb = TokenEmbedding(d_model, tgt_vocab_size)\n","        self.encoder = Encoder(d_model, d_ff, num_heads, num_encoder_layers, dropout)\n","        self.decoder = Decoder(d_model, d_ff, num_heads, num_decoder_layers, dropout)\n","        self.generator = nn.Linear(d_model, tgt_vocab_size)\n","    \n","    def forward(self, src_input_tensor, tgt_input_tensor, src_mask, tgt_mask, padding_mask=None):\n","        src_emb = self.pos_encoding(self.src_tok_emb(src_input_tensor))\n","        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt_input_tensor))\n","        output_encoder = self.encoder(src_emb, src_mask)\n","        output_decoder = self.decoder(tgt_emb, output_encoder, tgt_mask)\n","        return self.generator(output_decoder)\n","    \n","    def encode(self, src_input_tensor, src_mask):\n","        src_emb = self.pos_encoding(self.src_tok_emb(src_input_tensor))\n","        output_encoder = self.encoder(src_emb, src_mask)\n","        return output_encoder\n","    def decode(self, tgt_input_tensor, tgt_mask, memory):\n","        tgt_emb = self.pos_encoding(self.tgt_tok_emb(tgt_input_tensor))\n","        output_decoder = self.decoder(tgt_emb, memory, tgt_mask)\n","        return self.generator(output_decoder)"],"metadata":{"id":"LEViqwM51uog","execution":{"iopub.status.busy":"2023-05-02T06:54:56.474203Z","iopub.execute_input":"2023-05-02T06:54:56.474758Z","iopub.status.idle":"2023-05-02T06:54:56.487893Z","shell.execute_reply.started":"2023-05-02T06:54:56.474719Z","shell.execute_reply":"2023-05-02T06:54:56.487138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Model"],"metadata":{"id":"GIAuA4oEBkIR"}},{"cell_type":"markdown","source":["\n","### Utils"],"metadata":{"id":"BwSYPMTNKQVf"}},{"cell_type":"code","source":["def create_mask(batch, lang): \n","    assert lang == 'src' or lang == 'tgt'\n","    bsz, length = batch.size()\n","    mask = torch.ones(bsz, length, length)   \n","    # For source sequences, return a mask with all ones\n","    if lang == 'src':\n","        return mask.to(device)\n","    # For target sequences, return a mask that only attends to positions before the current position\n","    else:\n","        return torch.triu(mask).transpose(1, 2).to(device)\n","\n","def create_padding_mask(batch):\n","    bsz, length = batch.size()\n","    # The mask is 0 for padding tokens and 1 for all other tokens.\n","    padding_mask = (batch != PAD_IDX).unsqueeze(-1)\n","    return padding_mask.to(device)\n","\n","def create_std_mask(padding_mask, mask, position):\n","    assert position == 'encoder' or position == 'decoderI' or position == 'decoderII'\n","    # \"encoder\": Encoder don't attend to padding token in src sentence\n","    # \"decoderI\": Decoder don't attend to padding token in tgt sentence and only focus on positions before the current position\n","    # \"decoderII\": Decoder don't attend to padding token in src sentence \n","    if position == 'encoder' or position == 'decoderI':\n","        padding_mask = torch.bmm(padding_mask.float(), padding_mask.transpose(1, 2).float())\n","        mask = padding_mask * mask\n","    else:\n","        mask = torch.bmm(mask.float(), padding_mask.transpose(1, 2).float())\n","    return mask.long().to(device)\n"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.489056Z","iopub.execute_input":"2023-05-02T06:54:56.490058Z","iopub.status.idle":"2023-05-02T06:54:56.502875Z","shell.execute_reply.started":"2023-05-02T06:54:56.490023Z","shell.execute_reply":"2023-05-02T06:54:56.502078Z"},"trusted":true,"id":"XW86Wg1hKQVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Let's go bruhhh"],"metadata":{"id":"MAHwSLguKQVg"}},{"cell_type":"code","source":["def train_epoch(epoch, model, optimizer, scheduler):\n","    model.train()\n","    losses = 0\n","    total = 0\n","    for batch_id, (src, tgt) in enumerate(train_dataloader):\n","        src = src.transpose(0, 1).to(device)\n","        tgt = tgt.transpose(0, 1).to(device)\n","\n","        tgt_input = tgt[:, :-1]\n","        src_mask, tgt_mask = create_mask(src, 'src'), create_mask(tgt_input, 'tgt')\n","        src_padding_mask, tgt_padding_mask = create_padding_mask(src), create_padding_mask(tgt_input)\n","        src_mask = create_std_mask(src_padding_mask, src_mask, 'encoder').long()\n","        tgt_mask = create_std_mask(tgt_padding_mask, tgt_mask, 'decoderI').long()\n","        last_mask = create_std_mask(src_padding_mask, tgt_padding_mask, 'decoderII').long()\n","        \n","        logits = model(src, tgt_input, src_mask , tgt_mask, last_mask)\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[:, 1:]\n","        \n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","            \n","\n","        optimizer.step()\n","        scheduler.step()\n","        losses += loss.item()\n","        total += tgt.size(0)\n","        del tgt_input\n","        del tgt_mask\n","        del src_mask\n","        del last_mask\n","        del src_padding_mask\n","        del tgt_padding_mask\n","        del loss\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        if batch_id % 100 == 0 or batch_id < 10:\n","            print(f\"\"\"Total loss: {losses:.4f} | Total: {total} | Loss per batch: {losses/(batch_id + 1):.4f}\"\"\")\n","    \n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'loss': losses / len(train_dataloader),\n","    }\n","    if epoch > continue_training_from_checkpoint and epoch % 10 == 0:\n","        torch.save(checkpoint, f'/kaggle/working/checkpoint_{epoch}.pth')\n","    del checkpoint\n","    gc.collect()\n","    return losses / len(train_dataloader)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:54:56.504579Z","iopub.execute_input":"2023-05-02T06:54:56.504839Z","iopub.status.idle":"2023-05-02T06:54:56.519728Z","shell.execute_reply.started":"2023-05-02T06:54:56.504815Z","shell.execute_reply":"2023-05-02T06:54:56.518636Z"},"trusted":true,"id":"4pShMLkEKQVh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Optimize ram usage \n","del source_data\n","del target_data\n","del src_tokenized\n","del tgt_tokenized\n","del train_iter\n","gc.collect()\n","torch.cuda.empty_cache()\n","\n","transformer = Transformer(num_encoder_layers, num_decoder_layers, d_model, d_ff, src_vocab_size, tgt_vocab_size, num_heads, dropout)\n","transformer.to(device)\n","optimizer = torch.optim.Adam(transformer.parameters(), lr=2e-5, betas=(0.9, 0.98), eps=1e-9)\n","num_training_steps = len(train_dataloader) * NUM_EPOCHS\n","num_warmup_steps = int(num_training_steps * 0.1)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n","\n","if continue_training_from_checkpoint == 0:\n","    for p in transformer.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform_(p)\n","else:\n","    checkpoint = torch.load(checkpoint_path)\n","    transformer.load_state_dict(checkpoint['model_state_dict'])\n","    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","    scheduler.load_state_dict(checkpoint['scheduler_state_dict']) \n","    epoch = checkpoint['epoch']\n","    loss = checkpoint['loss']\n","    print(f\"Epoch: {epoch}, Avg train loss: {loss:.3f}\")\n","    \n","loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n","\n","for epoch in range(continue_training_from_checkpoint+1, NUM_EPOCHS+1):\n","    print(f'======== Epoch {epoch} / {NUM_EPOCHS} ========')\n","    print('Training...')\n","    start_time = time.time()\n","    train_loss = train_epoch(epoch, transformer, optimizer, scheduler)\n","    end_time = time.time()\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"],"metadata":{"id":"I_me1Hh8Jhgd","outputId":"883547d8-8a38-457a-afe1-bf5860f6f65e","execution":{"iopub.status.busy":"2023-05-02T06:54:56.523109Z","iopub.execute_input":"2023-05-02T06:54:56.523443Z","iopub.status.idle":"2023-05-02T06:55:10.547847Z","shell.execute_reply.started":"2023-05-02T06:54:56.523416Z","shell.execute_reply":"2023-05-02T06:55:10.546578Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch: 50, Avg train loss: 3.077\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Beam search"],"metadata":{"id":"Y0dl31L1CAZT"}},{"cell_type":"code","source":["def beam_search_decode(model, src, src_mask, max_len, beam_size):\n","    src = src.to(device)\n","    src_mask = src_mask.to(device)\n","\n","    memory = model.encode(src, src_mask)\n","\n","    # Initialize the beam with the start-of-sequence token\n","    beam = [(0.0, [BOS_IDX])]\n","\n","    # Iterate until the maximum length or the end-of-sequence token is generated\n","    for i in range(max_len-1):\n","        new_beam = []\n","#         print(f'Step: {i}')\n","#         print('----------------')\n","        # Generate candidates for each beam\n","        for score, sequence in beam:\n","            # Get the last generated token\n","            last_token = sequence[-1]\n","#             print(last_token)\n","\n","            # Stop generating if the end-of-sequence token is generated\n","            if last_token == EOS_IDX:\n","                new_beam.append((score, sequence))\n","                continue\n","\n","            # Decode the current sequence\n","            ys = torch.tensor(sequence).unsqueeze(0).to(device)\n","            tgt_mask = create_mask(ys, 'tgt').to(device)\n","            out = model.decode(ys, tgt_mask, memory)\n","            prob = out[:, -1]\n","\n","            # Get the top k candidates for the next token\n","            topk_scores, topk_indices = torch.topk(prob, k=beam_size)\n","\n","            # Add the top k candidates to the beam\n","            for j in range(beam_size):\n","                new_score = score + topk_scores[0, j].item()\n","                new_sequence = sequence + [topk_indices[0, j].item()]\n","                new_beam.append((new_score, new_sequence))\n","\n","        # Keep only the top k candidates in the beam\n","        beam = heapq.nlargest(beam_size, new_beam, key=lambda x: x[0])\n","#         print('------------')\n","\n","    # Return the sequence with the highest score in the beam\n","    return beam[0][1]\n"],"metadata":{"id":"4eLxiM8JHdjh","execution":{"iopub.status.busy":"2023-05-02T06:55:10.549655Z","iopub.execute_input":"2023-05-02T06:55:10.550002Z","iopub.status.idle":"2023-05-02T06:55:10.560341Z","shell.execute_reply.started":"2023-05-02T06:55:10.549965Z","shell.execute_reply":"2023-05-02T06:55:10.559189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def translate(model: torch.nn.Module, src_sentence: str):\n","    model.eval()\n","    src_token = word_tokenize(preprocess_sentence(src_sentence, SRC_LANGUAGE))\n","    src = torch.tensor(vocab_transform[SRC_LANGUAGE](src_token)).unsqueeze(0)\n","    num_tokens = src.size(-1)\n","    print(src.size())\n","    tgt_tokens = beam_search_decode(model, src, src_mask=create_mask(src, 'src'), max_len=num_tokens +5, beam_size=10)\n","    return (\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(tgt_tokens))).replace(\"<bos>\", \"<bos>\").replace(\"<eos>\", \"<eos>\").strip()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:55:10.562271Z","iopub.execute_input":"2023-05-02T06:55:10.562892Z","iopub.status.idle":"2023-05-02T06:55:10.575136Z","shell.execute_reply.started":"2023-05-02T06:55:10.562843Z","shell.execute_reply":"2023-05-02T06:55:10.574031Z"},"trusted":true,"id":"-FPOVTjvKQVi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(translate(transformer, 'science haha'))"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:55:10.577960Z","iopub.execute_input":"2023-05-02T06:55:10.578304Z","iopub.status.idle":"2023-05-02T06:55:12.971637Z","shell.execute_reply.started":"2023-05-02T06:55:10.578277Z","shell.execute_reply":"2023-05-02T06:55:12.970649Z"},"trusted":true,"id":"eNRpRy3VKQVi","outputId":"b12fbfd3-84d3-4766-f004-03b4b40ee843"},"execution_count":null,"outputs":[{"name":"stdout","text":"torch.Size([1, 2])\n<bos> ngành khoa_học ngừng khoa_học sử_dụng khoa_học\n","output_type":"stream"}]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"Z3AVmiDHKQVj"}},{"cell_type":"code","source":["del train_dataloader\n","gc.collect()"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:55:12.974354Z","iopub.execute_input":"2023-05-02T06:55:12.975024Z","iopub.status.idle":"2023-05-02T06:55:13.312408Z","shell.execute_reply.started":"2023-05-02T06:55:12.974983Z","shell.execute_reply":"2023-05-02T06:55:13.311239Z"},"trusted":true,"id":"GDIcP5PwKQVj","outputId":"905b002b-546f-4a57-a58a-e63c795e319b"},"execution_count":null,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"72"},"metadata":{}}]},{"cell_type":"code","source":["def calculate_bleu_score(model, data_loader):\n","    # Set the model to evaluation mode\n","    print(\"Bleu Score\")\n","    print(\"Calculating...\")\n","    model.eval()\n","\n","    # Initialize the references and hypotheses lists\n","    references = []\n","    hypotheses = []\n","\n","    # Iterate over the validation set and generate translations\n","    with torch.no_grad():\n","        for batch_id, (src, tgt) in enumerate(data_loader):\n","            if batch_id % 100 == 0:\n","                print(f\"Processed {batch_id}/{len(data_loader)}\")\n","            src = src.transpose(0,1).to(device)\n","            tgt = tgt.transpose(0,1).to(device)\n","            # Generate a translation using beam search decoding\n","            translation = beam_search_decode(model, src, src_mask=create_mask(src, 'src'), max_len=tgt.size(1) + 5, beam_size=10)\n","\n","            # Convert the tokens to strings and append to the lists\n","#             print([tgt.cpu().numpy().tolist()])\n","            references.extend([tgt.cpu().numpy().tolist()])\n","#             print(2)\n","            hypotheses.append(translation)\n","\n","    # Calculate the BLEU score\n","#     print((references))\n","#     print((hypotheses))\n","    bleu_score = corpus_bleu(references, hypotheses)\n","\n","    return bleu_score"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:55:13.313953Z","iopub.execute_input":"2023-05-02T06:55:13.315097Z","iopub.status.idle":"2023-05-02T06:55:13.324569Z","shell.execute_reply.started":"2023-05-02T06:55:13.315056Z","shell.execute_reply":"2023-05-02T06:55:13.323407Z"},"trusted":true,"id":"3fS3mu09KQVj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preparing Validation dataset\n","source_data, target_data = load_data(DATA_DIR+\"/tst2012.en\", DATA_DIR+\"/tst2012.vi\", 1e9, 1e9)\n","src_tokenized = [word_tokenize(i) for i in source_data]\n","tgt_tokenized = [ViTokenizer.tokenize(i).split() for i in target_data]\n","val_iter = [i for i in zip(src_tokenized, tgt_tokenized)]\n","val_dataloader = DataLoader(val_iter, batch_size=1, shuffle=True, collate_fn=collate_fn)\n","# Evaluation\n","calculate_bleu_score(transformer, val_dataloader)"],"metadata":{"execution":{"iopub.status.busy":"2023-05-02T06:55:13.331060Z","iopub.execute_input":"2023-05-02T06:55:13.331734Z","iopub.status.idle":"2023-05-02T07:29:14.315245Z","shell.execute_reply.started":"2023-05-02T06:55:13.331702Z","shell.execute_reply":"2023-05-02T07:29:14.314208Z"},"trusted":true,"id":"utMwZd2SKQVj","outputId":"49f7b3b9-1708-4127-8416-8918716358dd"},"execution_count":null,"outputs":[{"name":"stdout","text":"Bleu Score\nCalculating...\nProcessed 0/1553\nProcessed 100/1553\nProcessed 200/1553\nProcessed 300/1553\nProcessed 400/1553\nProcessed 500/1553\nProcessed 600/1553\nProcessed 700/1553\nProcessed 800/1553\nProcessed 900/1553\nProcessed 1000/1553\nProcessed 1100/1553\nProcessed 1200/1553\nProcessed 1300/1553\nProcessed 1400/1553\nProcessed 1500/1553\n","output_type":"stream"},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"0.07085401329851057"},"metadata":{}}]},{"cell_type":"code","source":["from nltk.translate.bleu_score import corpus_bleu\n","\n","# Define reference and candidate translations\n","references = [['the', 'cat', 'is', 'on', 'the', 'mat'],\n","              ['there', 'is', 'a', 'cat', 'on', 'the', 'mat']]\n","candidate = ['the', 'cat', 'is', 'on', 'the', 'mat']\n","\n","# Compute BLEU score between candidate and reference translations\n","bleu_score = corpus_bleu([references], [candidate])\n","\n","# Print BLEU score\n","print(bleu_score)\n"],"metadata":{"id":"Mka8zUX8PBLE","execution":{"iopub.status.busy":"2023-04-28T12:58:27.041212Z","iopub.execute_input":"2023-04-28T12:58:27.041593Z","iopub.status.idle":"2023-04-28T12:58:27.049003Z","shell.execute_reply.started":"2023-04-28T12:58:27.041555Z","shell.execute_reply":"2023-04-28T12:58:27.047960Z"},"trusted":true},"execution_count":null,"outputs":[]}]}